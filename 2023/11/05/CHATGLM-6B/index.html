<!DOCTYPE html><html lang="en" theme-mode="dark"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>CHATGLM-6B清华AI语言模型 | Notes|笔记站</title><link rel="icon" type="image/x-icon" href="/Arknight-notes/favicon.ico"><link rel="preload" as="font" crossorigin="anonymous" href="/Arknight-notes/font/Bender.ttf"><link rel="preload" as="font" crossorigin="anonymous" href="/Arknight-notes/font/BenderLight.ttf"><link rel="preload" as="font" crossorigin="anonymous" href="/Arknight-notes/font/JetBrainsMono-Regular.woff2"><link rel="stylesheet" href="/Arknight-notes/css/arknights.css"><style>@font-face {
  font-family: Bender;
  src: local('Bender'), url("/Arknight-notes/font/Bender.ttf"), url("/Arknight-notes/font/Bender.otf");
}
@font-face {
  font-family: BenderLight;
  src: local('BenderLight'), url("/Arknight-notes/font/BenderLight.ttf");
}
@font-face {
  font-family: 'JetBrains Mono';
  src: local('JetBrains Mono'), url('/Arknight-notes/font/JetBrainsMono-Regular.woff2') format('woff2');
}
</style><script>var config = {"root":"/Arknight-notes/","code_fold":90,"search":{"preload":false,"activeHolder":"Enter here","blurHolder":"Search","noResult":"Data \"$0\" not found"},"code":{"codeInfo":"$0 - $1 lines","copy":"copy"}}</script><link type="text/css" rel="stylesheet" href="/Arknight-notes/lib/encrypt/hbe.style.css"><script src="/Arknight-notes/js/gitalk.js"></script><script src="//unpkg.com/mermaid@10.5.0/dist/mermaid.min.js"></script><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lightgallery.css"><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lg-zoom.css"><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lg-thumbnail.css"><link type="text/css" rel="stylesheet" href="/Arknight-notes/lib/fontawesome/css/all.min.css"><script>if (window.localStorage.getItem('theme-mode') === 'light')
 document.documentElement.setAttribute('theme-mode', 'light')
if (window.localStorage.getItem('theme-mode') === 'dark')
 document.documentElement.setAttribute('theme-mode', 'dark')</script><style>@font-face {
 font-family: BenderLight;
 src: local('Bender'), url("/Arknight-notes/font/BenderLight.woff2") format('woff2');
}
@font-face {
 font-family: 'JetBrains Mono';
 src: local('JetBrains Mono'), url('/Arknight-notes/font/JetBrainsMono-Regular.woff2') format('woff2');
}</style><style>:root {
 --dark-background: url('https://pic2.zhimg.com/v2-6269d74b4dafe14781d03790e5a86b21_r.jpg');
 --light-background: url('https://pic1.zhimg.com/v2-233b64b583642fc4a6d77e69ced33150_r.jpg');
 --theme-encrypt-confirm: 'confirm'
}</style><script defer src="/Arknight-notes/js/arknights.js"></script><script defer src="/Arknight-notes/js/search.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script async src="/Arknight-notes/js/gitalk.js"></script><script defer type="module">import mermaid from '//unpkg.com/mermaid@10.5.0/dist/mermaid.esm.mjs';
window.mermaid = mermaid;
code.paintMermaid();
</script><script async src="//unpkg.com/lightgallery@2.7.1/lightgallery.min.js"></script><script async src="//unpkg.com/lightgallery@2.7.1/plugins/zoom/lg-zoom.min.js"></script><script async src="//unpkg.com/lightgallery@2.7.1/plugins/thumbnail/lg-thumbnail.min.js"></script><script async src="/Arknight-notes/lib/encrypt/hbe.js"></script><script async src="/Arknight-notes/js/pjax.js"></script><script class="pjax-js">reset= () => {gitalk = new Gitalk({
 clientID: 'Ov23ct13Fb0b67x23wnT',
 clientSecret: 'f18ce69dd545e3c0f5e2456afde9c756fe8a254a',
 repo: 'Arknight-notes',
 owner: 'Zhongye1',
 admin: ['Zhongye1'],
 distractionFreeMode: false,
 id: location.pathname
});
if (document.querySelector("#gitalk")) gitalk.render("gitalk");document.querySelector('.lg-container')?.remove()
lightGallery(document.getElementById('post-bg'), {
  plugins: [lgZoom,lgThumbnail],
  selector: '.item-img'})}</script><script>window.addEventListener("load",() => {pjax = new Pjax({
 cacheBust: false,
 selectors: ['title','article','#aside-block','.pjax-js','data-pjax','.busuanzi'],
 switches: {'article': Pjax.switches.sideBySide},
 switchesOptions: {
   'article': {
     classNames: {
       remove: "pjax-out",
       add: "pjax-in"
     }
   }
 }
});
document.addEventListener("pjax:complete", reset);document.addEventListener('pjax:success', _ => bszCaller.fetch(
 "//busuanzi.ibruce.info/busuanzi?jsonpCallback=BusuanziCallback", a => {
  bszTag.texts(a),
  bszTag.shows()
}));reset()})</script><script class="pjax-js">reset= () => {gitalk = new Gitalk({
 clientID: 'Ov23ct13Fb0b67x23wnT',
 clientSecret: 'f18ce69dd545e3c0f5e2456afde9c756fe8a254a',
 repo: 'Arknight-notes',
 owner: 'Zhongye1',
 admin: ['Zhongye1'],
 distractionFreeMode: false,
 id: location.pathname
});
if (document.querySelector("#gitalk")) gitalk.render("gitalk");document.querySelector('.lg-container')?.remove()
lightGallery(document.getElementById('post-bg'), {
  plugins: [lgZoom,lgThumbnail],
  selector: '.item-img'})}</script><script>window.addEventListener("load",() => {pjax = new Pjax({
 cacheBust: false,
 selectors: ['title','article','#aside-block','.pjax-js'],
 switches: {'article': Pjax.switches.sideBySide},
 switchesOptions: {
   'article': {
     classNames: {
       remove: "pjax-out",
       add: "pjax-in"
     }
   }
 }
});
document.addEventListener("pjax:complete", reset);reset()})</script><meta name="generator" content="Hexo 7.3.0"></head><body><div class="loading" style="opacity: 0;"><div class="loadingBar left"></div><div class="loadingBar right"></div></div><main><header class="closed"><div class="navBtn"><i class="navBtnIcon"><span class="navBtnIconBar"></span><span class="navBtnIconBar"></span><span class="navBtnIconBar"></span></i></div><nav><div class="navItem" id="search-header"><span class="navItemTitle"><input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="Search" spellcheck="false" maxlength="50" type="text" id="search-input"></span></div><div class="navItem" id="search-holder"></div><div class="search-popup" tabindex="0"><div id="search-result"></div></div><ol class="navContent"><li class="navItem"><a class="navBlock" href="/Arknight-notes/"><span class="navItemTitle">Home</span></a></li><li class="navItem" matchdata="categories,tags"><a class="navBlock" href="/Arknight-notes/archives/"><span class="navItemTitle">Archives</span></a></li><li class="navItem"><a class="navBlock" href="/Arknight-notes/about/"><span class="navItemTitle">About</span></a></li></ol></nav></header><article><div id="post-bg"><div id="post-title"><h1>CHATGLM-6B清华AI语言模型</h1><div id="post-info"><span>First Post: <div class="control"><time datetime="2023-11-05T06:39:51.000Z" id="date"> 2023-11-05</time></div></span><br><span>Last Update: <div class="control"><time datetime="2024-07-17T16:29:45.387Z" id="updated"> 2024-07-18</time></div></span><br><span id="busuanzi_container_page_pv">Page View: <span class="control" id="busuanzi_value_page_pv">loading...</span></span></div></div><hr><div id="post-content"><p><del>猜猜我在清华找到了什么好东西</del><br><del>模型可以装在电脑里跑的，然后发现电脑带不动（悲）</del></p>
<span id="more"></span>

<h1 id="ChatGLM-6B"><a href="#ChatGLM-6B" class="headerlink" title="ChatGLM-6B"></a>ChatGLM-6B</h1><h1 id="https-gitclone-com-aiit-chat"><a href="#https-gitclone-com-aiit-chat" class="headerlink" title="https://gitclone.com/aiit/chat/"></a><a target="_blank" rel="noopener" href="https://gitclone.com/aiit/chat/">https://gitclone.com/aiit/chat/</a></h1><p><del>页面嵌入各种bug</del></p>
<p>新一代开源模型 <a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM3">ChatGLM3-6B</a> 已发布，拥有10B以下最强的基础模型，支持工具调用（Function Call）、代码执行（Code Interpreter）、Agent 任务等功能。</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>ChatGLM-6B 是一个开源的、支持中英双语的对话语言模型，基于 <a target="_blank" rel="noopener" href="https://github.com/THUDM/GLM">General Language Model (GLM)</a> 架构，具有 62 亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署（INT4 量化级别下最低只需 6GB 显存）。<br>ChatGLM-6B 使用了和 ChatGPT 相似的技术，针对中文问答和对话进行了优化。经过约 1T 标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62 亿参数的 ChatGLM-6B 已经能生成相当符合人类偏好的回答，更多信息请参考我们的<a target="_blank" rel="noopener" href="https://chatglm.cn/blog">博客</a>。欢迎通过 <a target="_blank" rel="noopener" href="https://chatglm.cn/">chatglm.cn</a> 体验更大规模的 ChatGLM 模型。</p>
<p>为了方便下游开发者针对自己的应用场景定制模型，我们同时实现了基于 <a target="_blank" rel="noopener" href="https://github.com/THUDM/P-tuning-v2">P-Tuning v2</a> 的高效参数微调方法 <a href="ptuning/README.md">(使用指南)</a> ，INT4 量化级别下最低只需 7GB 显存即可启动微调。</p>
<p>ChatGLM-6B 权重对学术研究<strong>完全开放</strong>，在填写<a target="_blank" rel="noopener" href="https://open.bigmodel.cn/mla/form">问卷</a>进行登记后<strong>亦允许免费商业使用</strong>。</p>
<hr>
<p>ChatGLM-6B 开源模型旨在与开源社区一起推动大模型技术发展，恳请开发者和大家遵守<a href="MODEL_LICENSE">开源协议</a>，勿将开源模型和代码及基于开源项目产生的衍生物用于任何可能给国家和社会带来危害的用途以及用于任何未经过安全评估和备案的服务。<strong>目前，本项目团队未基于 ChatGLM-6B 开发任何应用，包括网页端、安卓、苹果 iOS 及 Windows App 等应用。</strong></p>
<p>尽管模型在训练的各个阶段都尽力确保数据的合规性和准确性，但由于 ChatGLM-6B 模型规模较小，且模型受概率随机性因素影响，无法保证输出内容的准确性，且模型易被误导（详见<a href="README.md#%E5%B1%80%E9%99%90%E6%80%A7">局限性</a>）。<strong>本项目不承担开源模型和代码导致的数据安全、舆情风险或发生任何模型被误导、滥用、传播、不当利用而产生的风险和责任。</strong></p>
<h2 id="更新信息"><a href="#更新信息" class="headerlink" title="更新信息"></a>更新信息</h2><p><strong>[2023&#x2F;07&#x2F;25]</strong> 发布 <a target="_blank" rel="noopener" href="https://github.com/THUDM/CodeGeeX2">CodeGeeX2</a> ，基于 ChatGLM2-6B 的代码生成模型，代码能力全面提升，更多特性包括：</p>
<ul>
<li><strong>更强大的代码能力</strong>：CodeGeeX2-6B 进一步经过了 600B 代码数据预训练，相比 CodeGeeX 一代模型，在代码能力上全面提升，<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/THUDM/humaneval-x">HumanEval-X</a> 评测集的六种编程语言均大幅提升 (Python +57%, C++ +71%, Java +54%, JavaScript +83%, Go +56%, Rust +321%)，在Python上达到 35.9% 的 Pass@1 一次通过率，超越规模更大的 StarCoder-15B。</li>
<li><strong>更优秀的模型特性</strong>：继承 ChatGLM2-6B 模型特性，CodeGeeX2-6B 更好支持中英文输入，支持最大 8192 序列长度，推理速度较一代 大幅提升，量化后仅需6GB显存即可运行，支持轻量级本地化部署。</li>
<li><strong>更全面的AI编程助手</strong>：CodeGeeX插件（<a target="_blank" rel="noopener" href="https://marketplace.visualstudio.com/items?itemName=aminer.codegeex">VS Code</a>, <a target="_blank" rel="noopener" href="https://plugins.jetbrains.com/plugin/20587-codegeex">Jetbrains</a>）后端升级，支持超过100种编程语言，新增上下文补全、跨文件补全等实用功能。结合 Ask CodeGeeX 交互式AI编程助手，支持中英文对话解决各种编程问题，包括且不限于代码解释、代码翻译、代码纠错、文档生成等，帮助程序员更高效开发。</li>
</ul>
<p><strong>[2023&#x2F;06&#x2F;25]</strong> 发布 <a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM2-6B">ChatGLM2-6B</a>，ChatGLM-6B 的升级版本，在保留了了初代模型对话流畅、部署门槛较低等众多优秀特性的基础之上，ChatGLM<strong>2</strong>-6B 引入了如下新特性：</p>
<ol>
<li><strong>更强大的性能</strong>：基于 ChatGLM 初代模型的开发经验，我们全面升级了 ChatGLM2-6B 的基座模型。ChatGLM2-6B 使用了 <a target="_blank" rel="noopener" href="https://github.com/THUDM/GLM">GLM</a> 的混合目标函数，经过了 1.4T 中英标识符的预训练与人类偏好对齐训练，<a href="#%E8%AF%84%E6%B5%8B%E7%BB%93%E6%9E%9C">评测结果</a>显示，相比于初代模型，ChatGLM2-6B 在 MMLU（+23%）、CEval（+33%）、GSM8K（+571%） 、BBH（+60%）等数据集上的性能取得了大幅度的提升，在同尺寸开源模型中具有较强的竞争力。</li>
<li><strong>更长的上下文</strong>：基于 <a target="_blank" rel="noopener" href="https://github.com/HazyResearch/flash-attention">FlashAttention</a> 技术，我们将基座模型的上下文长度（Context Length）由 ChatGLM-6B 的 2K 扩展到了 32K，并在对话阶段使用 8K 的上下文长度训练，允许更多轮次的对话。但当前版本的 ChatGLM2-6B 对单轮超长文档的理解能力有限，我们会在后续迭代升级中着重进行优化。</li>
<li><strong>更高效的推理</strong>：基于 <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1911.02150">Multi-Query Attention</a> 技术，ChatGLM2-6B 有更高效的推理速度和更低的显存占用：在官方的模型实现下，推理速度相比初代提升了 42%，INT4 量化下，6G 显存支持的对话长度由 1K 提升到了 8K。</li>
</ol>
<p>更多信息参见 <a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM2-6B">ChatGLM2-6B</a>。</p>
<p><strong>[2023&#x2F;06&#x2F;14]</strong> 发布 <a target="_blank" rel="noopener" href="https://github.com/THUDM/WebGLM">WebGLM</a>，一项被接受于KDD 2023的研究工作，支持利用网络信息生成带有准确引用的长回答。</p>
<p><strong>[2023&#x2F;05&#x2F;17]</strong> 发布 <a target="_blank" rel="noopener" href="https://github.com/THUDM/VisualGLM-6B">VisualGLM-6B</a>，一个支持图像理解的多模态对话语言模型。</p>
<p>可以通过本仓库中的 <a href="cli_demo_vision.py">cli_demo_vision.py</a> 和 <a href="web_demo_vision.py">web_demo_vision.py</a> 来运行命令行和网页 Demo。注意 VisualGLM-6B 需要额外安装 <a target="_blank" rel="noopener" href="https://github.com/THUDM/SwissArmyTransformer/">SwissArmyTransformer</a> 和 torchvision。更多信息参见 <a target="_blank" rel="noopener" href="https://github.com/THUDM/VisualGLM-6B">VisualGLM-6B</a>。</p>
<p><strong>[2023&#x2F;05&#x2F;15]</strong> 更新 v1.1 版本 checkpoint，训练数据增加英文指令微调数据以平衡中英文数据比例，解决英文回答中夹杂中文词语的现象。</p>
<h2 id="友情链接"><a href="#友情链接" class="headerlink" title="友情链接"></a>友情链接</h2><p>对 ChatGLM 进行加速的开源项目：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://huggingface.co/TMElyralab/lyraChatGLM">lyraChatGLM</a>: 对 ChatGLM-6B 进行推理加速，最高可以实现 9000+ tokens&#x2F;s 的推理速度</li>
<li><a target="_blank" rel="noopener" href="https://github.com/wangzhaode/ChatGLM-MNN">ChatGLM-MNN</a>: 一个基于 MNN 的 ChatGLM-6B C++ 推理实现，支持根据显存大小自动分配计算任务给 GPU 和 CPU</li>
<li><a target="_blank" rel="noopener" href="https://github.com/Jittor/JittorLLMs">JittorLLMs</a>：最低3G显存或者没有显卡都可运行 ChatGLM-6B FP16， 支持Linux、windows、Mac部署</li>
<li><a target="_blank" rel="noopener" href="https://github.com/MegEngine/InferLLM">InferLLM</a>：轻量级 C++ 推理，可以实现本地 x86，Arm 处理器上实时聊天，手机上也同样可以实时运行，运行内存只需要 4G</li>
</ul>
<p>基于或使用了 ChatGLM-6B 的开源项目：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://github.com/imClumsyPanda/langchain-ChatGLM">langchain-ChatGLM</a>：基于 langchain 的 ChatGLM 应用，实现基于可扩展知识库的问答</li>
<li><a target="_blank" rel="noopener" href="https://github.com/l15y/wenda">闻达</a>：大型语言模型调用平台，基于 ChatGLM-6B 实现了类 ChatPDF 功能</li>
<li><a target="_blank" rel="noopener" href="https://github.com/initialencounter/glm-bot">glm-bot</a>：将ChatGLM接入Koishi可在各大聊天平台上调用ChatGLM</li>
<li><a target="_blank" rel="noopener" href="https://github.com/GaiZhenbiao/ChuanhuChatGPT">Chuanhu Chat</a>: 为各个大语言模型和在线模型API提供美观易用、功能丰富、快速部署的用户界面，支持ChatGLM-6B。</li>
</ul>
<p>支持 ChatGLM-6B 和相关应用在线训练的示例项目：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.heywhale.com/mw/project/6436d82948f7da1fee2be59e">ChatGLM-6B 的部署与微调教程</a></li>
<li><a target="_blank" rel="noopener" href="https://www.heywhale.com/mw/project/643977aa446c45f4592a1e59">ChatGLM-6B 结合 langchain 实现本地知识库 QA Bot</a></li>
</ul>
<p>第三方评测：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2304.12986">Measuring Massive Multitask Chinese Understanding</a></li>
</ul>
<p>更多开源项目参见 <a href="PROJECT.md">PROJECT.md</a></p>
<h2 id="使用方式"><a href="#使用方式" class="headerlink" title="使用方式"></a>使用方式</h2><h3 id="硬件需求"><a href="#硬件需求" class="headerlink" title="硬件需求"></a>硬件需求</h3><table>
<thead>
<tr>
<th><strong>量化等级</strong></th>
<th><strong>最低 GPU 显存</strong>（推理）</th>
<th><strong>最低 GPU 显存</strong>（高效参数微调）</th>
</tr>
</thead>
<tbody><tr>
<td>FP16（无量化）</td>
<td>13 GB</td>
<td>14 GB</td>
</tr>
<tr>
<td>INT8</td>
<td>8 GB</td>
<td>9 GB</td>
</tr>
<tr>
<td>INT4</td>
<td>6 GB</td>
<td>7 GB</td>
</tr>
</tbody></table>
<h3 id="环境安装"><a href="#环境安装" class="headerlink" title="环境安装"></a>环境安装</h3><p>使用 pip 安装依赖：<code>pip install -r requirements.txt</code>，其中 <code>transformers</code> 库版本推荐为 <code>4.27.1</code>，但理论上不低于 <code>4.23.1</code> 即可。</p>
<p>此外，如果需要在 cpu 上运行量化后的模型，还需要安装 <code>gcc</code> 与 <code>openmp</code>。多数 Linux 发行版默认已安装。对于 Windows ，可在安装 <a target="_blank" rel="noopener" href="https://jmeubank.github.io/tdm-gcc/">TDM-GCC</a> 时勾选 <code>openmp</code>。 Windows 测试环境 <code>gcc</code> 版本为 <code>TDM-GCC 10.3.0</code>， Linux 为 <code>gcc 11.3.0</code>。在 MacOS 上请参考 <a href="FAQ.md#q1">Q1</a>。</p>
<h3 id="代码调用"><a href="#代码调用" class="headerlink" title="代码调用"></a>代码调用</h3><p>可以通过如下代码调用 ChatGLM-6B 模型来生成对话：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModel<br><span class="hljs-meta">&gt;&gt;&gt; </span>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;THUDM/chatglm-6b&quot;</span>, trust_remote_code=<span class="hljs-literal">True</span>)<br><span class="hljs-meta">&gt;&gt;&gt; </span>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;THUDM/chatglm-6b&quot;</span>, trust_remote_code=<span class="hljs-literal">True</span>).half().cuda()<br><span class="hljs-meta">&gt;&gt;&gt; </span>model = model.<span class="hljs-built_in">eval</span>()<br><span class="hljs-meta">&gt;&gt;&gt; </span>response, history = model.chat(tokenizer, <span class="hljs-string">&quot;你好&quot;</span>, history=[])<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(response)<br>你好👋!我是人工智能助手 ChatGLM-6B,很高兴见到你,欢迎问我任何问题。<br><span class="hljs-meta">&gt;&gt;&gt; </span>response, history = model.chat(tokenizer, <span class="hljs-string">&quot;晚上睡不着应该怎么办&quot;</span>, history=history)<br><span class="hljs-meta">&gt;&gt;&gt; </span><span class="hljs-built_in">print</span>(response)<br>晚上睡不着可能会让你感到焦虑或不舒服,但以下是一些可以帮助你入睡的方法:<br><br><span class="hljs-number">1.</span> 制定规律的睡眠时间表:保持规律的睡眠时间表可以帮助你建立健康的睡眠习惯,使你更容易入睡。尽量在每天的相同时间上床,并在同一时间起床。<br><span class="hljs-number">2.</span> 创造一个舒适的睡眠环境:确保睡眠环境舒适,安静,黑暗且温度适宜。可以使用舒适的床上用品,并保持房间通风。<br><span class="hljs-number">3.</span> 放松身心:在睡前做些放松的活动,例如泡个热水澡,听些轻柔的音乐,阅读一些有趣的书籍等,有助于缓解紧张和焦虑,使你更容易入睡。<br><span class="hljs-number">4.</span> 避免饮用含有咖啡因的饮料:咖啡因是一种刺激性物质,会影响你的睡眠质量。尽量避免在睡前饮用含有咖啡因的饮料,例如咖啡,茶和可乐。<br><span class="hljs-number">5.</span> 避免在床上做与睡眠无关的事情:在床上做些与睡眠无关的事情,例如看电影,玩游戏或工作等,可能会干扰你的睡眠。<br><span class="hljs-number">6.</span> 尝试呼吸技巧:深呼吸是一种放松技巧,可以帮助你缓解紧张和焦虑,使你更容易入睡。试着慢慢吸气,保持几秒钟,然后缓慢呼气。<br><br>如果这些方法无法帮助你入睡,你可以考虑咨询医生或睡眠专家,寻求进一步的建议。<br></code></pre></td></tr></table></figure>
<p>模型的实现仍然处在变动中。如果希望固定使用的模型实现以保证兼容性，可以在 <code>from_pretrained</code> 的调用中增加 <code>revision=&quot;v1.1.0&quot;</code> 参数。<code>v1.1.0</code> 是当前最新的版本号，完整的版本列表参见 <a target="_blank" rel="noopener" href="https://huggingface.co/THUDM/chatglm-6b#change-log">Change Log</a>。</p>
<h3 id="从本地加载模型"><a href="#从本地加载模型" class="headerlink" title="从本地加载模型"></a>从本地加载模型</h3><p>以上代码会由 <code>transformers</code> 自动下载模型实现和参数。完整的模型实现可以在 <a target="_blank" rel="noopener" href="https://huggingface.co/THUDM/chatglm-6b">Hugging Face Hub</a>。如果你的网络环境较差，下载模型参数可能会花费较长时间甚至失败。此时可以先将模型下载到本地，然后从本地加载。</p>
<p>从 Hugging Face Hub 下载模型需要先<a target="_blank" rel="noopener" href="https://docs.github.com/zh/repositories/working-with-files/managing-large-files/installing-git-large-file-storage">安装Git LFS</a>，然后运行</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Shell">git clone https://huggingface.co/THUDM/chatglm-6b<br></code></pre></td></tr></table></figure>

<p>如果你从 Hugging Face Hub 上下载 checkpoint 的速度较慢，可以只下载模型实现</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Shell">GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/THUDM/chatglm-6b<br></code></pre></td></tr></table></figure>
<p>然后从<a target="_blank" rel="noopener" href="https://cloud.tsinghua.edu.cn/d/fb9f16d6dc8f482596c2/">这里</a>手动下载模型参数文件，并将下载的文件替换到本地的 <code>chatglm-6b</code> 目录下。</p>
<p>将模型下载到本地之后，将以上代码中的 <code>THUDM/chatglm-6b</code> 替换为你本地的 <code>chatglm-6b</code> 文件夹的路径，即可从本地加载模型。</p>
<p><strong>Optional</strong> 模型的实现仍然处在变动中。如果希望固定使用的模型实现以保证兼容性，可以执行</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs Shell">git checkout v1.1.0<br></code></pre></td></tr></table></figure>

<h2 id="Demo-API"><a href="#Demo-API" class="headerlink" title="Demo &amp; API"></a>Demo &amp; API</h2><p>我们提供了一个基于 <a target="_blank" rel="noopener" href="https://gradio.app/">Gradio</a> 的网页版 Demo 和一个命令行 Demo。使用时首先需要下载本仓库：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">git clone https://github.com/THUDM/ChatGLM-6B<br>cd ChatGLM-6B<br></code></pre></td></tr></table></figure>

<h3 id="网页版-Demo"><a href="#网页版-Demo" class="headerlink" title="网页版 Demo"></a>网页版 Demo</h3><p class='item-img' data-src='/Arknight-notes/./CHATGLM-6B/resources/web-demo.gif'><img src="/Arknight-notes/./CHATGLM-6B/resources/web-demo.gif" alt="web-demo.gif"></p>
<p>首先安装 Gradio：<code>pip install gradio</code>，然后运行仓库中的 <a href="web_demo.py">web_demo.py</a>： </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">python web_demo.py<br></code></pre></td></tr></table></figure>

<p>程序会运行一个 Web Server，并输出地址。在浏览器中打开输出的地址即可使用。最新版 Demo 实现了打字机效果，速度体验大大提升。注意，由于国内 Gradio 的网络访问较为缓慢，启用 <code>demo.queue().launch(share=True, inbrowser=True)</code> 时所有网络会经过 Gradio 服务器转发，导致打字机体验大幅下降，现在默认启动方式已经改为 <code>share=False</code>，如有需要公网访问的需求，可以重新修改为 <code>share=True</code> 启动。</p>
<p>感谢 <a target="_blank" rel="noopener" href="https://github.com/AdamBear">@AdamBear</a> 实现了基于 Streamlit 的网页版 Demo，运行方式见<a target="_blank" rel="noopener" href="https://github.com/THUDM/ChatGLM-6B/pull/117">#117</a>.</p>
<h3 id="命令行-Demo"><a href="#命令行-Demo" class="headerlink" title="命令行 Demo"></a>命令行 Demo</h3><p class='item-img' data-src='/Arknight-notes/./CHATGLM-6B/resources/cli-demo.png'><img src="/Arknight-notes/./CHATGLM-6B/resources/cli-demo.png" alt="cli-demo.png"></p>
<p>运行仓库中 <a href="cli_demo.py">cli_demo.py</a>：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">python cli_demo.py<br></code></pre></td></tr></table></figure>

<p>程序会在命令行中进行交互式的对话，在命令行中输入指示并回车即可生成回复，输入 <code>clear</code> 可以清空对话历史，输入 <code>stop</code> 终止程序。</p>
<h3 id="API部署"><a href="#API部署" class="headerlink" title="API部署"></a>API部署</h3><p>首先需要安装额外的依赖 <code>pip install fastapi uvicorn</code>，然后运行仓库中的 <a href="api.py">api.py</a>：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">python api.py<br></code></pre></td></tr></table></figure>
<p>默认部署在本地的 8000 端口，通过 POST 方法进行调用</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">curl -X POST &quot;http://127.0.0.1:8000&quot; \<br>     -H &#x27;Content-Type: application/json&#x27; \<br>     -d &#x27;&#123;&quot;prompt&quot;: &quot;你好&quot;, &quot;history&quot;: []&#125;&#x27;<br></code></pre></td></tr></table></figure>
<p>得到的返回值为</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">&#123;<br>  &quot;response&quot;:&quot;你好👋！我是人工智能助手 ChatGLM-6B，很高兴见到你，欢迎问我任何问题。&quot;,<br>  &quot;history&quot;:[[&quot;你好&quot;,&quot;你好👋！我是人工智能助手 ChatGLM-6B，很高兴见到你，欢迎问我任何问题。&quot;]],<br>  &quot;status&quot;:200,<br>  &quot;time&quot;:&quot;2023-03-23 21:38:40&quot;<br>&#125;<br></code></pre></td></tr></table></figure>

<h2 id="低成本部署"><a href="#低成本部署" class="headerlink" title="低成本部署"></a>低成本部署</h2><h3 id="模型量化"><a href="#模型量化" class="headerlink" title="模型量化"></a>模型量化</h3><p>默认情况下，模型以 FP16 精度加载，运行上述代码需要大概 13GB 显存。如果你的 GPU 显存有限，可以尝试以量化方式加载模型，使用方法如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 按需修改，目前只支持 4/8 bit 量化</span><br>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;THUDM/chatglm-6b&quot;</span>, trust_remote_code=<span class="hljs-literal">True</span>).quantize(<span class="hljs-number">8</span>).half().cuda()<br></code></pre></td></tr></table></figure>

<p>进行 2 至 3 轮对话后，8-bit 量化下 GPU 显存占用约为 10GB，4-bit 量化下仅需 6GB 占用。随着对话轮数的增多，对应消耗显存也随之增长，由于采用了相对位置编码，理论上 ChatGLM-6B 支持无限长的 context-length，但总长度超过 2048（训练长度）后性能会逐渐下降。</p>
<p>模型量化会带来一定的性能损失，经过测试，ChatGLM-6B 在 4-bit 量化下仍然能够进行自然流畅的生成。使用 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2210.17323">GPT-Q</a> 等量化方案可以进一步压缩量化精度&#x2F;提升相同量化精度下的模型性能，欢迎大家提出对应的 Pull Request。</p>
<p>量化过程需要在内存中首先加载 FP16 格式的模型，消耗大概 13GB 的内存。如果你的内存不足的话，可以直接加载量化后的模型，INT4 量化后的模型仅需大概 5.2GB 的内存：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># INT8 量化的模型将&quot;THUDM/chatglm-6b-int4&quot;改为&quot;THUDM/chatglm-6b-int8&quot;</span><br>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;THUDM/chatglm-6b-int4&quot;</span>, trust_remote_code=<span class="hljs-literal">True</span>).half().cuda()<br></code></pre></td></tr></table></figure>
<p>量化模型的参数文件也可以从<a target="_blank" rel="noopener" href="https://cloud.tsinghua.edu.cn/d/674208019e314311ab5c/">这里</a>手动下载。</p>
<h3 id="CPU-部署"><a href="#CPU-部署" class="headerlink" title="CPU 部署"></a>CPU 部署</h3><p>如果你没有 GPU 硬件的话，也可以在 CPU 上进行推理，但是推理速度会更慢。使用方法如下（需要大概 32GB 内存）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model = AutoModel.from_pretrained(<span class="hljs-string">&quot;THUDM/chatglm-6b&quot;</span>, trust_remote_code=<span class="hljs-literal">True</span>).<span class="hljs-built_in">float</span>()<br></code></pre></td></tr></table></figure>

<p>如果你的内存不足，可以直接加载量化后的模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># INT8 量化的模型将&quot;THUDM/chatglm-6b-int4&quot;改为&quot;THUDM/chatglm-6b-int8&quot;</span><br>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;THUDM/chatglm-6b-int4&quot;</span>,trust_remote_code=<span class="hljs-literal">True</span>).<span class="hljs-built_in">float</span>()<br></code></pre></td></tr></table></figure>

<p>如果遇到了报错 <code>Could not find module &#39;nvcuda.dll&#39;</code> 或者 <code>RuntimeError: Unknown platform: darwin</code> (MacOS) ，请<a href="README.md#%E4%BB%8E%E6%9C%AC%E5%9C%B0%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B">从本地加载模型</a></p>
<h3 id="Mac-部署"><a href="#Mac-部署" class="headerlink" title="Mac 部署"></a>Mac 部署</h3><p>对于搭载了 Apple Silicon 或者 AMD GPU 的Mac，可以使用 MPS 后端来在 GPU 上运行 ChatGLM-6B。需要参考 Apple 的 <a target="_blank" rel="noopener" href="https://developer.apple.com/metal/pytorch">官方说明</a> 安装 PyTorch-Nightly（正确的版本号应该是2.1.0.dev2023xxxx，而不是2.0.0）。</p>
<p>目前在 MacOS 上只支持<a href="README.md#%E4%BB%8E%E6%9C%AC%E5%9C%B0%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B">从本地加载模型</a>。将代码中的模型加载改为从本地加载，并使用 mps 后端：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model = AutoModel.from_pretrained(<span class="hljs-string">&quot;your local path&quot;</span>, trust_remote_code=<span class="hljs-literal">True</span>).half().to(<span class="hljs-string">&#x27;mps&#x27;</span>)<br></code></pre></td></tr></table></figure>

<p>加载半精度的 ChatGLM-6B 模型需要大概 13GB 内存。内存较小的机器（比如 16GB 内存的 MacBook Pro），在空余内存不足的情况下会使用硬盘上的虚拟内存，导致推理速度严重变慢。此时可以使用量化后的模型如 chatglm-6b-int4。因为 GPU 上量化的 kernel 是使用 CUDA 编写的，因此无法在 MacOS 上使用，只能使用 CPU 进行推理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># INT8 量化的模型将&quot;THUDM/chatglm-6b-int4&quot;改为&quot;THUDM/chatglm-6b-int8&quot;</span><br>model = AutoModel.from_pretrained(<span class="hljs-string">&quot;THUDM/chatglm-6b-int4&quot;</span>,trust_remote_code=<span class="hljs-literal">True</span>).<span class="hljs-built_in">float</span>()<br></code></pre></td></tr></table></figure>
<p>为了充分使用 CPU 并行，还需要<a href="FAQ.md#q1">单独安装 OpenMP</a>。</p>
<h3 id="多卡部署"><a href="#多卡部署" class="headerlink" title="多卡部署"></a>多卡部署</h3><p>如果你有多张 GPU，但是每张 GPU 的显存大小都不足以容纳完整的模型，那么可以将模型切分在多张GPU上。首先安装 accelerate: <code>pip install accelerate</code>，然后通过如下方法加载模型：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> utils <span class="hljs-keyword">import</span> load_model_on_gpus<br>model = load_model_on_gpus(<span class="hljs-string">&quot;THUDM/chatglm-6b&quot;</span>, num_gpus=<span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure>
<p>即可将模型部署到两张 GPU 上进行推理。你可以将 <code>num_gpus</code> 改为你希望使用的 GPU 数。默认是均匀切分的，你也可以传入 <code>device_map</code> 参数来自己指定。 </p>
<h2 id="高效参数微调"><a href="#高效参数微调" class="headerlink" title="高效参数微调"></a>高效参数微调</h2><p>基于 <a target="_blank" rel="noopener" href="https://github.com/THUDM/P-tuning-v2">P-tuning v2</a> 的高效参数微调。具体使用方法详见 <a href="ptuning/README.md">ptuning&#x2F;README.md</a>。</p>
<h2 id="ChatGLM-6B-示例"><a href="#ChatGLM-6B-示例" class="headerlink" title="ChatGLM-6B 示例"></a>ChatGLM-6B 示例</h2><p>以下是一些使用 <code>web_demo.py</code> 得到的示例截图。<br class='item-img' data-src='/Arknight-notes/./CHATGLM-6B/examples/ad-writing-2.png'><img src="/Arknight-notes/./CHATGLM-6B/examples/ad-writing-2.png" alt="ad-writing-2.png"><br class='item-img' data-src='/Arknight-notes/./CHATGLM-6B/examples/blog-outline.png'><img src="/Arknight-notes/./CHATGLM-6B/examples/blog-outline.png" alt="blog-outline.png"><br class='item-img' data-src='/Arknight-notes/./CHATGLM-6B/examples/comments-writing.png'><img src="/Arknight-notes/./CHATGLM-6B/examples/comments-writing.png" alt="comments-writing.png"><br class='item-img' data-src='/Arknight-notes/./CHATGLM-6B/examples/email-writing-1.png'><img src="/Arknight-notes/./CHATGLM-6B/examples/email-writing-1.png" alt="email-writing-1.png"><br class='item-img' data-src='/Arknight-notes/./CHATGLM-6B/examples/email-writing-2.png'><img src="/Arknight-notes/./CHATGLM-6B/examples/email-writing-2.png" alt="email-writing-2.png"><br class='item-img' data-src='/Arknight-notes/./CHATGLM-6B/examples/information-extraction.png'><img src="/Arknight-notes/./CHATGLM-6B/examples/information-extraction.png" alt="information-extraction.png"><br class='item-img' data-src='/Arknight-notes/./CHATGLM-6B/examples/role-play.png'><img src="/Arknight-notes/./CHATGLM-6B/examples/role-play.png" alt="role-play.png"><br class='item-img' data-src='/Arknight-notes/./CHATGLM-6B/examples/self-introduction.png'><img src="/Arknight-notes/./CHATGLM-6B/examples/self-introduction.png" alt="self-introduction.png"><br class='item-img' data-src='/Arknight-notes/./CHATGLM-6B/examples/sport.png'><img src="/Arknight-notes/./CHATGLM-6B/examples/sport.png" alt="sport.png"><br class='item-img' data-src='/Arknight-notes/./CHATGLM-6B/examples/tour-guide.png'><img src="/Arknight-notes/./CHATGLM-6B/examples/tour-guide.png" alt="tour-guide.png"></p>
<h2 id="局限性"><a href="#局限性" class="headerlink" title="局限性"></a>局限性</h2><p>由于 ChatGLM-6B 的小规模，其能力仍然有许多局限性。以下是我们目前发现的一些问题：</p>
<ul>
<li><p>模型容量较小：6B 的小容量，决定了其相对较弱的模型记忆和语言能力。在面对许多事实性知识任务时，ChatGLM-6B 可能会生成不正确的信息；它也不擅长逻辑类问题（如数学、编程）的解答。</p>
<p class='item-img' data-src='/Arknight-notes/./CHATGLM-6B/limitations/factual_error.png'><img src="/Arknight-notes/./CHATGLM-6B/limitations/factual_error.png" alt="factual_error.png"><br class='item-img' data-src='/Arknight-notes/./CHATGLM-6B/limitations/math_error.png'><img src="/Arknight-notes/./CHATGLM-6B/limitations/math_error.png" alt="math_error.png"></p>
</li>
<li><p>产生有害说明或有偏见的内容：ChatGLM-6B 只是一个初步与人类意图对齐的语言模型，可能会生成有害、有偏见的内容。（内容可能具有冒犯性，此处不展示）</p>
</li>
<li><p>英文能力不足：ChatGLM-6B 训练时使用的指示&#x2F;回答大部分都是中文的，仅有极小一部分英文内容。因此，如果输入英文指示，回复的质量远不如中文，甚至与中文指示下的内容矛盾，并且出现中英夹杂的情况。</p>
</li>
<li><p>易被误导，对话能力较弱：ChatGLM-6B 对话能力还比较弱，而且 “自我认知” 存在问题，并很容易被误导并产生错误的言论。例如当前版本的模型在被误导的情况下，会在自我认知上发生偏差。</p>
<p class='item-img' data-src='/Arknight-notes/./CHATGLM-6B/limitations/self-confusion_google.jpg'><img src="/Arknight-notes/./CHATGLM-6B/limitations/self-confusion_google.jpg" alt="self-confusion_google.jpg"></p>
</li>
</ul>
<p class='item-img' data-src='/Arknight-notes/./CHATGLM-6B/limitations/self-confusion_openai.jpg'><img src="/Arknight-notes/./CHATGLM-6B/limitations/self-confusion_openai.jpg" alt="self-confusion_openai.jpg"><br class='item-img' data-src='/Arknight-notes/./CHATGLM-6B/limitations/self-confusion_tencent.jpg'><img src="/Arknight-notes/./CHATGLM-6B/limitations/self-confusion_tencent.jpg" alt="self-confusion_tencent.jpg"></p>
<h2 id="协议"><a href="#协议" class="headerlink" title="协议"></a>协议</h2><p>本仓库的代码依照 <a href="LICENSE">Apache-2.0</a> 协议开源，ChatGLM-6B 模型的权重的使用则需要遵循 <a href="MODEL_LICENSE">Model License</a>。ChatGLM-6B 权重对学术研究<strong>完全开放</strong>，在填写<a target="_blank" rel="noopener" href="https://open.bigmodel.cn/mla/form">问卷</a>进行登记后<strong>亦允许免费商业使用</strong>。</p>
<h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p>如果你觉得我们的工作有帮助的话，请考虑引用下列论文</p>
<figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">@article&#123;zeng2022glm,<br>  title=&#123;Glm<span class="hljs-number">-130</span>b: An open <span class="hljs-keyword">bilingual </span>pre-trained model&#125;,<br>  author=&#123;Zeng, Aohan <span class="hljs-keyword">and </span>Liu, Xiao <span class="hljs-keyword">and </span>Du, Zhengxiao <span class="hljs-keyword">and </span>Wang, Zihan <span class="hljs-keyword">and </span>Lai, Hanyu <span class="hljs-keyword">and </span><span class="hljs-keyword">Ding, </span>Ming <span class="hljs-keyword">and </span>Yang, Zhuoyi <span class="hljs-keyword">and </span>Xu, Yifan <span class="hljs-keyword">and </span>Zheng, Wendi <span class="hljs-keyword">and </span>Xia, Xiao <span class="hljs-keyword">and </span>others&#125;,<br>  <span class="hljs-keyword">journal=&#123;arXiv </span>preprint arXiv:<span class="hljs-number">2210</span>.<span class="hljs-number">02414</span>&#125;,<br>  year=&#123;<span class="hljs-number">2022</span>&#125;<br>&#125;<br></code></pre></td></tr></table></figure>
<figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">@inproceedings&#123;du2022glm,<br>  title=&#123;GLM: General Language Model Pretraining with Autoregressive <span class="hljs-keyword">Blank </span>Infilling&#125;,<br>  author=&#123;Du, Zhengxiao <span class="hljs-keyword">and </span>Qian, Yujie <span class="hljs-keyword">and </span>Liu, Xiao <span class="hljs-keyword">and </span><span class="hljs-keyword">Ding, </span>Ming <span class="hljs-keyword">and </span>Qiu, <span class="hljs-keyword">Jiezhong </span><span class="hljs-keyword">and </span>Yang, Zhilin <span class="hljs-keyword">and </span>Tang, <span class="hljs-keyword">Jie&#125;,</span><br><span class="hljs-keyword"></span>  <span class="hljs-keyword">booktitle=&#123;Proceedings </span>of the <span class="hljs-number">60</span>th Annual Meeting of the Association for Computational Linguistics (Volume <span class="hljs-number">1</span>: Long Papers)&#125;,<br>  pages=&#123;<span class="hljs-number">320</span>--<span class="hljs-number">335</span>&#125;,<br>  year=&#123;<span class="hljs-number">2022</span>&#125;<br>&#125;<br></code></pre></td></tr></table></figure>

<div id="paginator"></div></div><div id="post-footer"><div id="pages" style="justify-content: flex-start"><div class="footer-link" style="width: 50%;text-align:right;border-right:1px #fe2 solid"><a href="/Arknight-notes/2024/06/25/2024-06-25-LiteLoaderQQNT/">← Next LiteLoaderQQNT</a></div></div></div><div id="comments"><div id="gitalk"></div></div></div><div class="bottom-btn"><div><a class="i-top" id="to-top" onClick="scrolls.scrolltop();" title="To Top" style="opacity: 0; display: none;">∧ </a><a class="i-index" id="to-index" href="#toc-div" title="To Catalog">≡</a><a class="i-color" id="color-mode" onClick="colorMode.change()" title="Change Theme"></a></div></div></article><aside><div id="about"><a href="/Arknight-notes/" id="logo"><img src="https://pic4.zhimg.com/80/v2-d9884f32711e19e80979eac58e943897_720w.webp" alt="Logo" style="margin:20;border-radius:0;"></a><h1 id="Dr"><a href="zhongye">柊野</a></h1><div id="description"><p></p></div></div><div id="aside-block"><div id="toc-div"><h1>Catalog</h1><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#ChatGLM-6B"><span class="toc-number">1.</span> <span class="toc-text">ChatGLM-6B</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#https-gitclone-com-aiit-chat"><span class="toc-number">2.</span> <span class="toc-text">https:&#x2F;&#x2F;gitclone.com&#x2F;aiit&#x2F;chat&#x2F;</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8D"><span class="toc-number">2.1.</span> <span class="toc-text">介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9B%B4%E6%96%B0%E4%BF%A1%E6%81%AF"><span class="toc-number">2.2.</span> <span class="toc-text">更新信息</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%8B%E6%83%85%E9%93%BE%E6%8E%A5"><span class="toc-number">2.3.</span> <span class="toc-text">友情链接</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E6%96%B9%E5%BC%8F"><span class="toc-number">2.4.</span> <span class="toc-text">使用方式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A1%AC%E4%BB%B6%E9%9C%80%E6%B1%82"><span class="toc-number">2.4.1.</span> <span class="toc-text">硬件需求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85"><span class="toc-number">2.4.2.</span> <span class="toc-text">环境安装</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E8%B0%83%E7%94%A8"><span class="toc-number">2.4.3.</span> <span class="toc-text">代码调用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8E%E6%9C%AC%E5%9C%B0%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.4.4.</span> <span class="toc-text">从本地加载模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Demo-API"><span class="toc-number">2.5.</span> <span class="toc-text">Demo &amp; API</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BD%91%E9%A1%B5%E7%89%88-Demo"><span class="toc-number">2.5.1.</span> <span class="toc-text">网页版 Demo</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%91%BD%E4%BB%A4%E8%A1%8C-Demo"><span class="toc-number">2.5.2.</span> <span class="toc-text">命令行 Demo</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#API%E9%83%A8%E7%BD%B2"><span class="toc-number">2.5.3.</span> <span class="toc-text">API部署</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%8E%E6%88%90%E6%9C%AC%E9%83%A8%E7%BD%B2"><span class="toc-number">2.6.</span> <span class="toc-text">低成本部署</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96"><span class="toc-number">2.6.1.</span> <span class="toc-text">模型量化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CPU-%E9%83%A8%E7%BD%B2"><span class="toc-number">2.6.2.</span> <span class="toc-text">CPU 部署</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Mac-%E9%83%A8%E7%BD%B2"><span class="toc-number">2.6.3.</span> <span class="toc-text">Mac 部署</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E5%8D%A1%E9%83%A8%E7%BD%B2"><span class="toc-number">2.6.4.</span> <span class="toc-text">多卡部署</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%AB%98%E6%95%88%E5%8F%82%E6%95%B0%E5%BE%AE%E8%B0%83"><span class="toc-number">2.7.</span> <span class="toc-text">高效参数微调</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ChatGLM-6B-%E7%A4%BA%E4%BE%8B"><span class="toc-number">2.8.</span> <span class="toc-text">ChatGLM-6B 示例</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B1%80%E9%99%90%E6%80%A7"><span class="toc-number">2.9.</span> <span class="toc-text">局限性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8D%8F%E8%AE%AE"><span class="toc-number">2.10.</span> <span class="toc-text">协议</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%95%E7%94%A8"><span class="toc-number">2.11.</span> <span class="toc-text">引用</span></a></li></ol></li></ol></div></div><footer><nobr><span class="icp-title">GZHU</span><span class="icp-content">193001-0001</span></nobr><br><nobr><span class="icp-title">OUTPOST</span><span class="icp-content">169-2025-0331</span></nobr><br><nobr>Published with <a target="_blank" rel="noopener" href="http://hexo.io">Hexo</a></nobr><wbr><nobr> Theme <a target="_blank" rel="noopener" href="https://github.com/Yue-plus/hexo-theme-arknights">Arknights</a></nobr><wbr><nobr> by <a target="_blank" rel="noopener" href="https://github.com/Yue-plus">Yue_plus</a></nobr></footer></aside></main><canvas id="canvas-dust"></canvas></body></html>